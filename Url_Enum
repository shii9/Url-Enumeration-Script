# Professional URL Enumeration Methodology

**Version:** 1.1
**Category:** Web Application Security / Bug Bounty
**Objective:** To systematically collect, normalize, and analyze URLs to identify potential vulnerabilities. This guide is designed to be understood by team members of all skill levels.

---

## Phase 1: Data Collection
**Goal:** Gather as many URLs as possible from various sources. We use multiple tools because each tool has different strengths (e.g., some crawl active pages, others check historical archives).

### 1.1. GAU (Get All Urls)
**Purpose:** GAU fetches known URLs from alienvault, wayback machine, and common crawl. It is excellent for finding "forgotten" endpoints.

**Standard Scan:**
This command fetches every known URL for the domain and its subdomains.
```bash
gau example.com --subs --o all_urls.txt
```
*   `--subs`: Include subdomains (e.g., `api.example.com`).
*   `--o`: Save output to a file.

**Live URL Scan:**
This filters the results to only show URLs that are currently responding (Live).
```bash
gau example.com --subs --mc 200,301,302,403 --o live_urls.txt
```
*   `--mc`: Match Code. Only keep results with these HTTP status codes (200 OK, 3xx Redirects, 403 Forbidden).

**High-Speed Scan:**
Use this if you have a fast connection and want results quickly.
```bash
gau example.com --subs --threads 50 --o fast_urls.txt
```
*   `--threads 50`: Increases parallel connections to 50 (default is usually lower).

### 1.2. GoSpider
**Purpose:** A powerful crawler that visits the site and extracts URLs from the HTML and JavaScript files it finds.
```bash
gospider -s https://example.com --subs --js -d 3 -o all_urls
```
*   `-s`: The site to start crawling.
*   `--subs`: Include subdomains found during crawling.
*   `--js`: Specifically look inside JavaScript files for links.
*   `-d 3`: Depth 3. It will follow links 3 clicks deep from the homepage.

### 1.3. Katana
**Purpose:** A next-generation crawler that is very fast and good at handling modern JavaScript-heavy applications (Single Page Apps).

**Standard Crawl:**
```bash
katana -u https://example.com -d 3 -o urls.txt
```
*   `-u`: Target URL.
*   `-d 3`: Crawl depth.

**JavaScript Extraction:**
Focuses solely on finding `.js` files, which often contain API keys or hidden endpoints.
```bash
katana -u https://example.com -f js -o js_files.txt
```
*   `-f js`: Filter to only output JavaScript files.

### 1.4. Waymore
**Purpose:** Similar to GAU but often finds *more* results because it downloads the actual archived responses to find links that aren't indexed.
```bash
waymore -i example.com -mode 3 -o output.txt
```
*   `-i`: Input domain.
*   `-mode 3`: The most aggressive mode (fetches from all available sources).

### 1.5. Hakrawler
**Purpose:** A simple, fast crawler designed to be piped into other tools.
```bash
echo https://target.com | hakrawler -u -d 3
```
*   `-u`: Show unique URLs only.

---

## Phase 2: Specialized Extraction
**Goal:** Filter the massive list of URLs to find specific "high value" targets immediately.

**1. Finding Parameters (SQLi / XSS Targets)**
Looks for URLs containing `?` and `=`, which indicates user input is being handled.
```bash
grep -E '\?.+='
```

**2. Finding Sensitive Parameters (SSRF / RCE)**
Looks for specific parameter names like `token`, `url`, or `redirect` which are often vulnerable.
```bash
grep -E '\?(token|redirect|url)='
```

**3. Historical Analysis**
Focuses on a specific date range. Useful for finding old files that developers thought were deleted.
```bash
gau ... --from 201801 --to 202312
```

**4. JavaScript Secrets**
Searches inside gathered files for keywords like "api_key" or "secret".
```bash
grep -Ei "(api_key|secret|token)"
```

---

## Phase 3: Data Processing & Normalization
**Goal:** Raw data is messy. It contains duplicates, garbage images, and broken links. This pipeline cleans it up so you don't waste time scanning useless URLs.

### 3.1. The Normalization Pipeline
Copy and paste this entire block into your terminal. It takes your raw `merged_raw.txt` and produces a clean `normalized_urls.txt`.

```bash
cat merged_raw.txt \
  | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' \
  | sed '/^$/d' \
  | unfurl format '%scheme://%userinfo%host%port%path%query%fragment' 2>/dev/null \
  | sed 's://:/:g' \
  | sed 's/\/\{2,\}/\//g' \
  | sed 's/\/$//' \
  | grep -Ev '\.(jpg|jpeg|png|gif|bmp|svg|ico|css|eot|ttf|woff|woff2|pdf|zip|rar|tar)$' \
  | sort -u \
  > normalized_urls.txt
```

### 3.2. Detailed Explanation of the Pipeline
*   **`sed 's/^[[:space:]]*//...'`**: Trims spaces from the start and end of lines.
*   **`sed '/^$/d'`**: Deletes empty lines.
*   **`unfurl format ...`**: The most important step. It reconstructs every URL into a standard format (`http://site.com/path?query`). This fixes broken URLs.
*   **`sed 's://:/:g'`**: Fixes protocol errors (e.g., changes `http:///` to `http://`).
*   **`sed 's/\/\{2,\}/\//g'`**: Removes duplicate slashes in the path (e.g., `/api//v1` becomes `/api/v1`).
*   **`grep -Ev ...`**: "Invert Match". It removes any line ending in `.jpg`, `.png`, `.css`, etc. We don't need to scan images.
*   **`sort -u`**: Sorts the list and removes duplicates.

---

## Phase 4: Segmentation & Analysis
**Goal:** Now that we have clean data, we split it into "Attack Buckets" to run specific tests.

### 4.1. API Endpoints
**Why:** APIs are often less protected than the main website. We look for `/api/`, `/v1/`, or `.json` extensions.
```bash
grep -Ei '/api/|/graphql|/rest/|/v[0-9]+/|\.json($|\?)|/api-v|/api/v' normalized_urls.txt \
  | grep -Ev '\.(jpg|png|css|woff|pdf)$' \
  | uro \
  | sort -u > api_urls.txt
```

### 4.2. Auth & Admin Panels
**Why:** These are the "keys to the kingdom". We look for login pages, admin panels, and dashboards.
```bash
grep -Ei 'login|logout|signin|signup|admin|dashboard|wp-admin|manage|panel|console|auth|oauth' normalized_urls.txt \
  | uro \
  | sort -u > auth_admin_urls.txt
```

### 4.3. Parameter Fuzzing
**Why:** We want to test every parameter for vulnerabilities. We replace the value with `FUZZ` so tools like `ffuf` know where to inject payloads.

**Step 1: Extract & Clean**
```bash
grep '\?' normalized_urls.txt | sort -u | uro > urls_with_params.txt
```

**Step 2: Prepare for Fuzzing**
```bash
cat urls_with_params.txt | qsreplace 'FUZZ' > param_fuzz_list.txt
```

**Step 3: Run Fuzzing**
```bash
ffuf -u FUZZ -w param_fuzz_list.txt
```

### 4.4. JavaScript & Secrets
**Why:** Developers often accidentally leave credentials in client-side code.

**Step 1: Find JS Files**
```bash
grep -Ei '\.js($|\?)' normalized_urls.txt | sort -u > js_files.txt
```

**Step 2: Download Them**
```bash
mkdir -p harvested_js
cat js_files.txt | xargs -n1 -P25 -I{} sh -c 'curl -sL "{}" -o harvested_js/$(echo {} | sed -E "s/[:\/?#=&]/_/g")'
```

**Step 3: Hunt for Secrets**
```bash
grep -rhoE "https?://[^\"'\\)\\s]+" harvested_js/* | sort -u > js_extracted_urls.txt
```

---

## Phase 5: Live Verification
**Goal:** Confirm which of our discovered URLs are actually alive and responding.

```bash
cat endpoints.txt | httpx -threads 50 -silent -status-code -title -content-length -o httpx_results.txt
```
*   **`-status-code`**: Shows if the page is 200 OK, 403 Forbidden, etc.
*   **`-title`**: Prints the page title (e.g., "Login Page" vs "404 Not Found").
*   **`-content-length`**: Helps identify "soft 404s" (pages that say 200 OK but are actually empty).
