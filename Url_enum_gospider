#!/usr/bin/env bash
# url_collect_gospider
# Usage:
#   ./url_collect_gospider <target> [commands_file] > final_urls.txt
# Example:
#   ./url_collect_gospider example.com commands.txt > all_urls.txt
#
# If commands_file is omitted the script runs a sensible default set of gospider commands.
# In commands_file each line is a shell command template using {target} as placeholder.
# Lines starting with # or empty lines are ignored.

set -euo pipefail
IFS=$'\n\t'

if [[ $# -lt 1 ]]; then
  >&2 echo "Usage: $0 <target> [commands_file]  (redirect stdout to desired output file)"
  exit 2
fi

TARGET="$1"
CMDFILE="${2:-}"

# check gospider exists
if ! command -v gospider >/dev/null 2>&1; then
  >&2 echo "ERROR: 'gospider' not found in PATH. Install gospider and retry."
  exit 3
fi

TMPDIR="$(mktemp -d /tmp/url_collect_gospider.XXXX)"
trap 'rm -rf "$TMPDIR"' EXIT

run_cmd_save() {
  local idx="$1"
  local cmd="$2"
  local out="$TMPDIR/step_${idx}.txt"
  local stepdir="$TMPDIR/step_${idx}_out"

  mkdir -p "$stepdir"
  # replace {target} placeholder
  cmd="${cmd//\{target\}/$TARGET}"

  >&2 echo "âŸ¶ running step ${idx}: $cmd"
  # run the command inside the step directory so -o produces output there
  (cd "$stepdir" && bash -c "$cmd") >"$out" 2>/dev/null || true

  # collect any plain stdout urls in $out already, now extract urls from any files produced
  # find all files and run grep to extract urls, append to out
  # we use -print0 to safely handle filenames with spaces
  find "$stepdir" -type f -print0 \
    | xargs -0 -I{} grep -Eo 'https?://[^\"'"'"' \)\]\}<>]+' {} >>"$out" 2>/dev/null || true
  # also try to extract urls from HTML by looking for src/href patterns (best-effort)
  find "$stepdir" -type f -print0 \
    | xargs -0 -I{} sed -n 's/.*href="\([^"]\+\)".*/\1/p; s/.*src="\([^"]\+\)".*/\1/p' {} >>"$out" 2>/dev/null || true
}

# load commands
declare -a CMDS
if [[ -n "$CMDFILE" ]]; then
  if [[ ! -f "$CMDFILE" ]]; then
    >&2 echo "ERROR: commands file '$CMDFILE' not found."
    exit 4
  fi
  while IFS= read -r line || [[ -n "$line" ]]; do
    # strip whitespace
    line="${line#"${line%%[![:space:]]*}"}"
    line="${line%"${line##*[![:space:]]}"}"
    [[ -z "$line" || "${line:0:1}" == "#" ]] && continue
    CMDS+=("$line")
  done <"$CMDFILE"
else
  # default gospider commands (use {target} placeholder)
  CMDS+=("gospider -s https://{target} --subs --js -d 3 -o all_urls")
  CMDS+=("gospider -s https://{target} --subs -a -w -r -o full_recon")
  CMDS+=("gospider -s https://{target} -c 30 -t 10 -d 2 -o fast_spider")
  CMDS+=("gospider -s https://{target} --js --sitemap --robots -o structured_output")
  CMDS+=("gospider -S targets.txt -t 10 -c 20 -d 3 -o multi_scan")
  CMDS+=("gospider -s https://{target} --js --subs -a -w -r -d 3 -o ultimate")
  CMDS+=("gospider -s https://{target} --whitelist \"admin|auth|debug|panel\" -o filtered_admin")
  CMDS+=("gospider -s https://{target} --blacklist \"\\.(png|jpg|jpeg|css|svg|gif)$\"")
  CMDS+=("gospider -s https://{target} --subs --js --sitemap --robots -a -w -r -c 30 -d 3 -o final_spider")
fi

# run commands sequentially
count=0
for c in "${CMDS[@]}"; do
  count=$((count + 1))
  run_cmd_save "$count" "$c"
done

# consolidate outputs: combine temp step files, normalize urls, remove blanks/fragments, dedupe and sort
# extract http(s) urls only using grep to be safe
cat "$TMPDIR"/step_*.txt 2>/dev/null \
  | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' \
  | sed '/^$/d' \
  | sed 's/#.*$//' \
  | grep -Eo 'https?://[^\"'\'' )]+' || true \
  | sort -u \
  | awk '!seen[$0]++{print}' \
  | sort \
  || true

# summary (stderr)
{
  echo "----- summary -----"
  echo "target: $TARGET"
  echo "commands run: ${#CMDS[@]}"
  echo "raw outputs saved to: $TMPDIR"
  echo "per-step counts:"
  for f in "$TMPDIR"/step_*.txt; do
    n=$(wc -l <"$f" 2>/dev/null || echo 0)
    echo "  $(basename "$f"): $n lines"
  done
  echo "-------------------"
} >&2

# end
